---
title: "COLA RR analysis plan"
output: html_notebook
---

Analysis script illustrated with simulated data.  
For each part of analysis, we first show the text from the RR, and then illustrate the analysis with a simulated dataset.

```{r Rpackage_setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("ggstatsplot")
library(ggstatsplot)
library(tidyverse)
library(MASS)
library(MBESS)
library(nlme)
library(semPower)
library(bookdown)
library(lavaan)
library(semPlot)
# library(knitr)
library(flextable)
library(officer)
library(corrr) #added by DB for easy correlations
library(plyr)
library(qpcR) #used in Kievit script
library(kableExtra)
library(ggpubr)
options(scipen=999)
```


## Simulated data
To test our analysis, we create a simulated dataset, which is roughly based on what we know about distributions of the LIs based on prior research.

Two datasets will be simulated for 200 Lhanders and 200 Rhanders on three online laterality tasks. The simulated data are LIs - i.e. these are already computed at this stage.
Dataset N is entirely null. There is no lateral bias, all variables are just random normal deviates, and there is no group difference.
For dataset P, we add biases in line with our predictions, i.e. strong lateral bias for dichotic and rhyme tasks, weaker bias for the OVP task. Furthermore, for left-handers, a higher proportion are right-lateralised, leading to a lower mean and higher variance, and correlations between measures are weaker.

```{r simdata}
set.seed(3)
mytasks<-c('ovp','dichotic','rhyme')
#Null data have no bias and all estimates are just random numbers
mymeans <- c(0,0,0)
mySigma <- matrix(c(1,0,0,0,1,0,0,0,1),nrow=3)
myN <- 400

mydat <- mvrnorm(myN,mymeans,mySigma)
Lhand <- c(rep(0,200),rep(1,200))
mydatN <-data.frame(cbind(Lhand,mydat))
colnames(mydatN)[2:4]<-mytasks

head(mydatN)
correlate(mydatN) #check correlations, should be null

#for datasetP, we specify a new correlation matrix which is different for L and R handers
#we also specify separate means for the two groups.
mydatS <- mydatN #start by just reusing the null dataframe.

mymeansR <- c(.25,.5,.5) #positive is left-lateralised; more extreme for Dichotic and Rhyme
mymeansL <- (mymeansR - .25) #L handers are unbiased on OVP, and have mean lat of .25 on Dichotic and rhyme

mySigmaR <- matrix(c(1,.3,.3,.3,1,.6,.3,.6,1),nrow=3)
#modest correlation between dichotic and rhyme; significant but lower for OVP
mySigmaL <- matrix(c(1,.1,.1,.1,1,.3,.1,.3,1),nrow=3) #same corr between dichotic/rhyme for L handers, but lower with OVP
myN <- 200

mydatR <- mvrnorm(myN,mymeansR,mySigmaR)
mydatL <- mvrnorm(myN,mymeansL,mySigmaL)
mydatS[1:200,2:4]<-mydatR
mydatS[201:400,2:4]<-mydatL
correlate(mydatR)
correlate(mydatL)
correlate(mydatS) #both together

```
## Mixture model version
There is some evidence that to model the difference between left- and right-handers, we don't just need a lower mean for L handers: rather we should model as a mixture of left- and right-brain-biased cases, with a higher proportion of the latter among left-handers.
Easiest way to model this is to model with everyone left-brain-biased, but then just flip the sign for a proportion of individuals, which is higher for left-handers.
We'll call this mydatM.  

```{r simdataM}
mydatM <- mydatN #start by just reusing the null dataframe.

mymeans <- c(.5,.75,.75) #positive is left-lateralised; more extreme for Dichotic and Rhyme
#Because some will be flipped, need more extreme values here to be realistic

mySigma <- matrix(c(1,.3,.3,.3,1,.6,.3,.6,1),nrow=3)
#modest correlation between dichotic and rhyme; significant but lower for OVP
myN = 400
mydat <- mvrnorm(myN,mymeans,mySigma)
correlate(mydat)

#now flip polarity for proportion of individuals.
mydatM[2:4]<-mydat

pL <- .35  #proportion of L handers who flip
pR <- .05 #proportion of R handers who flip

for (j in 2:4){ #do this separately for each task
for (i in 1:myN){
  myp <- pR
  if(i>199){myp <- pL}
  if(runif(1)<myp) {mydatM[i,j]<-(-mydatM[i,j])}
}
}

correlate(mydatM)

```

Alternate approach - whether flipped or not depends on the individual; i.e. retain correlation in the flipped cases.  This is mydatF.  
```{r simdataF}
mydatF <- mydatN #start by just reusing the null dataframe.

mymeans <- c(.5,.75,.75) #positive is left-lateralised; more extreme for Dichotic and Rhyme
#Because some will be flipped, need more extreme values here to be realistic

mySigma <- matrix(c(1,.3,.3,.3,1,.6,.3,.6,1),nrow=3)
#modest correlation between dichotic and rhyme; significant but lower for OVP
myN = 400
mydat <- mvrnorm(myN,mymeans,mySigma)
correlate(mydat)

#now flip polarity for proportion of individuals.
mydatF[2:4]<-mydat

pL <- .35  #proportion of L handers who flip
pR <- .05 #proportion of R handers who flip


for (i in 1:myN){
  myp <- pR
  if(i>199){myp <- pL}
  if(runif(1)<myp) {mydatF[i,2:4]<-(-mydatF[i,2:4])}
  }

correlate(mydatF)

```
## Step 0. 
__Basic descriptives__  
Distributions of data will be visualised in scatterplots, showing pairwise associations between the three LI measures, with handedness colour-coded. As well as providing a table with means and SDs of all variables, we will compute split-half reliability for the online battery measures by correlating LIs obtained with alternate blocks of items for each task. Spearman correlations will be used to avoid inflated reliability from unduly influential datapoints. As noted in the text, reliability of each measure will constrain how strongly it correlates with the other measures.


## Step 1. 
__Prediction: On the online battery, dichotic and visual-half-field tasks will show significant left-brain lateralisation at the group level, with this effect being stronger in right- than left-handers.__    

To examine lateralisation at the brain level for each task will conduct the following analyses. Following Parker et al. (2020), we will conduct Shapiro-Wilk tests to check whether the distributions are normal for each measure. If distributions are normal, we will conduct a one-sample t-test to assess whether LIs at the group level are different from zero; otherwise a a Wilcoxon one-sample v test will be used.

The main analysis of the behavioural laterality indices will follow the approach used by Bruckert et al (2020), using Multilevel Linear Modelling (MLM) to assess the influence of task and handedness on lateralisation strength (LI). This method allows us to test whether the heterogeneity of variance is comparable in left- and right-handers, as well as quantifying main effects of task and handedness. MLM will be conducted within R (R Core Team, 2019) using the nlme package (version 3.1-148; Pinheiro et al., 2020). As per Bruckert et al. two models will be fitted to the data. Model 1 will assume homogeneous variance model assumptions (i.e. variances between subjects and groups are equal for the two handedness groups). Model 2 will assume heterogeneous between-subject variance by group (i.e. different variances for the left and right handed group). Model fit will be assessed using a likelihood ratio test comparing Model 1 and Model 2. Coefficients for the fixed and random effects will be reported for the best fitting model. Both models will include fixed effects of handedness, task and their interaction: nlme::lme(fixed = LI ~ handedness*task, random = list(id = pdDiag(form= ~ 0 + handedness))).  

We will adopt the following contrasts. For handedness, the contr.sum function from the base stats package will be used to implement summed-to-zero contrasts. This will correspond to a main effect of handedness. Helmert contrasts will be used for the task variable which compares each categorical level to the mean of the subsequent levels.

## Test for normality

```{r shapirowilk}

for (d in 1:4){ #each dataset
  thisdat<-mydatN
  mylabel<-'Null data'
   if(d==2){thisdat<-mydatS
    mylabel<-'meanshift data'}
    if(d==3){thisdat<-mydatM
     mylabel<-'Mixed model data'}
      if(d==4){thisdat<-mydatF
     mylabel<-'Flipped% data'}
  for (t in 2:4){ #each test
    s<-shapiro.test(thisdat[,t])$p.value
    print(paste0(mylabel,':',colnames(thisdat)[t],' Normality test, p-value: ',round(s,3)))
    }
}

```

 

## Test whether population mean is different from zero  

```{r onesamplet}

for (d in 1:4){ #each dataset
  thisdat<-mydatN
  mylabel<-'Null data'
   if(d==2){thisdat<-mydatS
    mylabel<-'meanshift data'}
    if(d==3){thisdat<-mydatM
     mylabel<-'Mixed model data'}
      if(d==4){thisdat<-mydatF
     mylabel<-'Flipped% data'}
  for (t in 2:4){ #each test
    x<-t.test(thisdat[,t])
    print(paste0(mylabel,':',colnames(thisdat)[t],': mean = ',round(x$estimate,3),', t=', round(x$statistic,3),', p-value = ',round(x$p.value,4)))
    }
}

```

Check distributions comparing L and R handers

```{r densfunction}
# Density plot 
densplotfunc<-function(myfile,mymain){

p <- ggplot(myfile, aes(x=thiscol,color=Lhand)) + 
    geom_density(alpha=.4)+
  ggtitle(mymain) # for the main title
xlab('LI') # for the x axis label
ylab('density') # for the y axis label

# Add mean lines

mu <- ddply(myfile, "Lhand", summarise, grp.mean=mean(thiscol))
p <- p+geom_vline(data=mu, aes(xintercept=grp.mean,color=Lhand), linetype="dashed")
return(p)
}
```

```{r doplots}
#run function for each measure

for (d in 1:4){ #each dataset
  thisdat<-mydatN
  mylabel<-'Null data'
   if(d==2){thisdat<-mydatS
    mylabel<-'meanshift data'}
    if(d==3){thisdat<-mydatM
     mylabel<-'Mixed model data'}
      if(d==4){thisdat<-mydatF
     mylabel<-'Flipped% data'}

thisdat$Lhand<-as.factor(thisdat$Lhand)
for (col in 2:4){ #do for each variable
  thisplot<-thisplot+1
  myfile2<-thisdat[,c(1,col)]
  mymain<-colnames(thisdat)[col]
  colnames(myfile2)[2]<-'thiscol'
  p<-densplotfunc(myfile2,mymain)
  t<-t.test(myfile2$thiscol~myfile2$Lhand)
  mymeans<-round(by(myfile2$thiscol,myfile2$Lhand,mean),3)
  mysds<-round(by(myfile2$thiscol,myfile2$Lhand,sd),3)
  print(paste0('Model ',mylabel,' --- ',mymain,": t=",round(t$statistic,2),",p = ",round(t$p.value,3)))
  print(paste0('Mean Rhander = ',mymeans[1],' SD = ',mysds[1], '.  Mean Lhander = ',mymeans[2],' SD = ',mysds[2]))
#  ggarrange(myg1,myg2,myg3,myg4,myg5,myg6,myg7,myg8, ncol = 2, nrow = 4,common.legend=T,legend="bottom")
# ggsave(
#   "all.raw.png",
#   width = 5, height = 8,
#   dpi = 300
  plotname<-paste0('scatter',thisplot,'.png')
  ggsave(plotname)
}
}
```



## Multilevel linear model

```{r MLL}
for (d in 1:4){ #each dataset
  thisdat<-mydatN
  mylabel<-'Null data'
   if(d==2){thisdat<-mydatS
    mylabel<-'meanshift data'}
    if(d==3){thisdat<-mydatM
     mylabel<-'Mixed model data'}
      if(d==4){thisdat<-mydatF
     mylabel<-'Flipped% data'}
  thisdat$id<-as.factor(1:400)
#First convert files to long form
longdat <- thisdat %>% gather("task", "LI", -c(id,Lhand))

longdat$task<-as.factor(longdat$task)
longdat$hand<-as.factor(longdat$Lhand)

# Nohand model - this is overkill really, just used here to demonstrate way to test if hand influences model
modn<-lme(fixed=LI~task, 
          random=list(id=pdSymm(form=~1)),
          data=longdat, 
          method="REML",contrasts=c(task='contr.helmert'))
print(mylabel) 
print(summary(modn))

# Homogeneous (base model)
mod0<-lme(fixed=LI~hand*task, 
          random=list(id=pdSymm(form=~1)),
          data=longdat, 
          method="REML",contrasts=c(hand='contr.sum',task='contr.helmert'))
print(mylabel) 
print(summary(mod0))

#Likelihood ratio test
LRtest1 <- anova(modn,mod0)
print(LRtest1)
# Heterogeneous (alternative model)

mod1<-lme(fixed=LI ~ hand*task, 
          random=list(id=pdDiag(form= ~ 0 + hand)),
          data=longdat,
          method="REML",contrasts=c(hand='contr.sum',task='contr.helmert'))

sum.mod1<-summary(mod1)
print(summary(mod1))

#Likelihood ratio test

LRtest <- anova(mod0,mod1)
print('Homogeneous vs heterogeneous model')
print(LRtest)
}
```

## Step 2
__2. The pattern of correlation between laterality indices from online measures will reflect the extent to which they involve implicit speech production, rather than whether they involve spoken or written language. Thus we anticipate dissociation between the rhyme judgement task and the other two measures (dichotic listening and OVP task), which is not accountable for in terms of low reliability of measures.__ 

For prediction 2 we focus on predicting patterns of covariance between the three online tasks, as we do not have sufficient indicators for modeling with latent variables. We prespecify four possible covariance structures, which are then compared using AIC weights. This is a subset of SEM that does not include any latent variables or directional paths. It allows us to constrain particular covariance patterns and report the 'best' model according to AIC weights.

The four models are:  
Model A, where all LIs are intercorrelated to a similar degree
Model B1,  where LIs for the two receptive language measures (dichotic listening and optimal viewing task) are intercorrelated, but independent of rhyme detection
Model B2, where LIs for the two tasks involving visual presentation and written language (optimal viewing and rhyme detection) are intercorrelated, and independent of the auditory task, dichotic listening.
Model C, where all LIs are independent of one another.

Models B1 and B2 are mathematically equivalent, differing only in the specific variables that are correlated. On a priori grounds, we favour model B1, which is compatible with our overall 2-factor model of language lateralisation. We mention B2, however, as this pattern of correlation might occur if the laterality index was dependent more on mode of presentation (visual vs auditory) than on task demands. In the section below on sample size justification, we outline the syntax used for this method and show that our sample size is adequate to distinguish between models.  Note that where we specify tests as correlated in a model, the extent of correlation will be constrained by test reliability. If the reliability of any of our measures is lower than .5, this would limit the interpretation of the models. 

```{r aicmodels}
#Specify models

  #Model A all correlated
    modelA<-"
    rhyme~~dichotic
    rhyme~~ovp
    dichotic~~ovp
    "
    #Model B 2 correlated
    modelB<-"
    rhyme~~dichotic
    rhyme~~0*ovp
    dichotic~~0*ovp"
    
    #model C all independent
    modelC<-"
    rhyme~~0*dichotic
    rhyme~~0*ovp
    dichotic~~0*ovp"

    modelBB<-"
    rhyme~~0*dichotic
    rhyme~~ovp
    dichotic~~0*ovp"
    
```    

```{r runAIC}
AICdf <- data.frame(matrix(NA,nrow=9,ncol=8))
colnames(AICdf)<-c('dataset','hand','modelA','modelB','modelC','modelBB','bestfit','chi.p.vs.C')
handgps <- c('All','L.only','R.only')
thisrow<-0


for (d in 1:4){ #d specifies which dataset we are using

  thisdat<-mydatN
  mylabel<-'Null data'
   if(d==2){thisdat<-mydatS
    mylabel<-'meanshift data'}
    if(d==3){thisdat<-mydatM
     mylabel<-'Mixed model data'}
      if(d==4){thisdat<-mydatF
     mylabel<-'Flipped% data'}
  for (h in 1:3){ #handedness grouping - all, L only, R only}
    
    
     if(h==2){thisdat<-thisdatall[201:400,]} #left-handers only
    if(h==3){thisdat<-thisdatall[1:200,]} #right-handers only  

    thisrow<-thisrow+1
 
    fitA <- cfa(modelA, data=thisdat) #
    fitB <- cfa(modelB, data=thisdat)
    fitC <- cfa(modelC, data=thisdat)
    fitBB <- cfa(modelBB,data=thisdat)
    
   aic_vector1<- c(fitMeasures(fitA, "aic"),fitMeasures(fitB, "aic"),fitMeasures(fitC, "aic"),fitMeasures(fitBB,"aic"))
    
    AICdf[thisrow,3:6]<-round(akaike.weights(aic_vector1)$weights,3)
   bestfit <- which(AICdf[thisrow,3:6]==max(AICdf[thisrow,3:6]))
     AICdf$bestfit[thisrow] <- LETTERS[bestfit]
     AICdf$hand[thisrow]<-handgps[h]
     AICdf$dataset[thisrow] <- mylabel

    
 if(bestfit<3)
 {
   compfit<-fitA
   if(bestfit==2){compfit<-fitB}
#If true model is not C, can it be distinguished from model C (all independent)
    AICdf$chi.p.vs.C[thisrow] <- round(anova(compfit,fitC)$`Pr(>Chisq)`[2],3)
 }
 }#end of h loop
} #end of d loop


flextable(AICdf)
```

# FTCD measures  

We need simulated data to assess the analyses, so we start by simulating the 6 measures, assuming a 2-factor model, and again using the 'flip' approach to simulation, which assumes that there are 2 underlying distributions, left-biased and right-biased, with a higher proportion of L-handers being in the atypical group.

```{r simdopF}
mydatF <- mydatN #start by just reusing the null dataframe.

mymeans <- c(.5,.75,.75) #positive is left-lateralised; more extreme for Dichotic and Rhyme
#Because some will be flipped, need more extreme values here to be realistic

mySigma <- matrix(c(1,.3,.3,.3,1,.6,.3,.6,1),nrow=3)
#modest correlation between dichotic and rhyme; significant but lower for OVP
myN = 400
mydat <- mvrnorm(myN,mymeans,mySigma)
correlate(mydat)

#now flip polarity for proportion of individuals.
mydatF[2:4]<-mydat

pL <- .35  #proportion of L handers who flip
pR <- .05 #proportion of R handers who flip


for (i in 1:myN){
  myp <- pR
  if(i>199){myp <- pL}
  if(runif(1)<myp) {mydatF[i,2:4]<-(-mydatF[i,2:4])}
  }

correlate(mydatF)

```


## Step 3
__The data will fit a model where 'language generation' tasks cluster together on one factor, and 'receptive languageâ€™ tasks on a second factor.__  

It is predicted that factors will be correlated, but the fit of a 2-factor model will be superior to a single-factor model where all LIs load on a common factor.
 
The analysis conducted by Woodhead et al (2019, 2020) used an exploratory bifactor model in which each task could load on each of two factors. Because we had two measures for each task (from test and retest sessions), this exploratory approach was adequately powered. The current study will use confirmatory factor analysis, using a prespecified two-factor model which constrains which indicators can load on two factors. This will be compared to a unitary model, in which all tasks load on a single factor.
 
If the fit of the two-factor model is poor, we will divide the sample into two random halves, before proceeding to drop or add paths from the model in Figure 4 to improve fit. When the optimal model has been identified, it will then be tested in confirmatory factor analysis using the hold-out portion of the data

